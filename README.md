# Projeto de Aprendizado de MÃ¡quina com Redes Multicamadas ğŸ§ 
---
### Autor
- **Nome:** Gustavo Alves de Oliveira
- **MatrÃ­cula:** 12311ECP026
- **Disciplina:** Aprendizagem de MÃ¡quina
- **Faculdade:** Universidade Federal de UberlÃ¢ndia


ğŸ¤–ğŸ“ŠğŸ”

> Este projeto faz uso da base de dados [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) para aprender a classificar flores em trÃªs tipos, sendo elas **Iris Setosa**, **Iris Versicolor** e **Iris Virginica**.\
As flores possuem quatro caracterÃ­sticas principais que as diferenciam, que sÃ£o: **comprimento da sÃ©pala**, **largura da sÃ©pala**, **comprimento da pÃ©tala** e **largura da pÃ©tala**.\
Foram utilizadas estratÃ©gias de otimizaÃ§Ã£o, como o cÃ¡lculo do **momento** para evitar oscilaÃ§Ãµes no modelo, ou seja, a funÃ§Ã£o de erro tender para o mÃ­nimo global de modo mais veloz; usamos tambÃ©m a estratÃ©gia de **validaÃ§Ã£o**, sendo uma possÃ­vel ponto de parada do treinamento, ajudando a ganhar generalizaÃ§Ã£o no modelo e evitando o **overtraining**; **normalizaÃ§Ã£o** dos dados de entrada, para valores entre 1.0 e -1.0.\



### Redes Neurais Perceptrons de Multicamadas (MLP)

> As Redes Neurais Perceptrons de Multicamadas (MLPs) sÃ£o uma classe de redes neurais artificiais que consistem em uma rede de nÃ³s (neurÃ´nios) organizados em camadas. As MLPs sÃ£o compostas por uma camada de entrada, uma ou mais camadas ocultas e uma camada de saÃ­da.

### Funcionamento das MLPs:

1. **Camada de Entrada:**
   - Os nÃ³s na camada de entrada representam as caracterÃ­sticas (features) dos dados de entrada. Cada nÃ³ nesta camada corresponde a uma caracterÃ­stica especÃ­fica.

2. **Camadas Ocultas:**
   - As camadas ocultas sÃ£o compostas por neurÃ´nios que processam os dados de entrada e aprendem a representaÃ§Ã£o dos padrÃµes nos dados.
   - Cada neurÃ´nio em uma camada oculta recebe entradas das camadas anteriores, realiza uma combinaÃ§Ã£o linear das entradas ponderadas por pesos, aplica uma funÃ§Ã£o de ativaÃ§Ã£o nÃ£o linear e passa o resultado para as camadas posteriores.
   - A presenÃ§a de camadas ocultas permite que as MLPs aprendam representaÃ§Ãµes mais complexas e nÃ£o lineares dos dados.

3. **Camada de SaÃ­da:**
   - A camada de saÃ­da produz a saÃ­da final da rede neural. A estrutura desta camada depende do tipo de problema que estÃ¡ sendo abordado, como classificaÃ§Ã£o (por exemplo, softmax para classificaÃ§Ã£o multiclasse) ou regressÃ£o (um Ãºnico neurÃ´nio de saÃ­da para prever um valor contÃ­nuo).

>![MLP](https://www.researchgate.net/publication/293013889/figure/fig1/AS:335717596188674@1457052720824/Figura-1-Exemplo-simplificado-de-uma-rede-neural-multicamadas-HAYKIN-2001-Figure-1.png)

### IdealizaÃ§Ã£o e Desenvolvimento:

>As MLPs nÃ£o tÃªm um Ãºnico idealizador. O desenvolvimento das MLPs foi uma evoluÃ§Ã£o das redes neurais perceptrons originais propostas por Frank Rosenblatt em 1957. As MLPs foram introduzidas para superar a limitaÃ§Ã£o das redes perceptrons de uma Ãºnica camada, que sÃ³ podiam resolver problemas linearmente separÃ¡veis. A ideia de adicionar camadas ocultas e usar algoritmos de treinamento como o backpropagation permitiu que as MLPs aprendessem a representar relaÃ§Ãµes mais complexas nos dados.

### FunÃ§Ã£o SigmÃ³ide Bipolar como FunÃ§Ã£o de AtivaÃ§Ã£o:

>A funÃ§Ã£o sigmÃ³ide bipolar, tambÃ©m conhecida como funÃ§Ã£o de ativaÃ§Ã£o bipolar, Ã© uma funÃ§Ã£o matemÃ¡tica usada em redes neurais como uma funÃ§Ã£o de ativaÃ§Ã£o. Sua fÃ³rmula Ã©:

$
    f(x) = \frac{2}{1 + e^{-x}} - 1
$

>Esta funÃ§Ã£o mapeia os valores de entrada para o intervalo [-1, 1]. Ela Ã© suave e diferenciÃ¡vel em todos os pontos, o que a torna adequada para o treinamento de redes neurais utilizando algoritmos de otimizaÃ§Ã£o baseados em gradiente, como o backpropagation.\
A funÃ§Ã£o sigmÃ³ide bipolar tem sido historicamente utilizada como funÃ§Ã£o de ativaÃ§Ã£o nos neurÃ´nios das camadas ocultas das MLPs. No entanto, devido a problemas como o desaparecimento do gradiente durante o treinamento profundo e a propagaÃ§Ã£o do gradiente muito lenta em camadas profundas, funÃ§Ãµes de ativaÃ§Ã£o alternativas, como ReLU (Rectified Linear Unit) e suas variantes, tornaram-se mais populares em redes neurais profundas modernas.

> ![SigmÃ³ide Bipolar](https://www.researchgate.net/publication/331087209/figure/fig4/AS:726046831820800@1550114462005/Figura-54-Funcion-de-Activacion-Sigmoide-Bipolar.jpg)

### Estrutura do Projeto

O projeto estÃ¡ estruturado da seguinte forma:

- **./data**: Possui os dados para **treino** (105 | 35 de cada flor), **teste** (15 | 5 de cada flor) e para **validaÃ§Ã£o** (30 | 10 de cada flor);
- **./src**: ContÃ©m os cÃ³digos-fonte;
- **./tmp**: IrÃ¡ conter os arquivos de relatÃ³rios dos pesos iniciais, pesos finais, valores do erro conforme a rede aprende e um arquivo **.html** que plota o grÃ¡fico da mÃ©dia quadrÃ¡tica.

### DependÃªncias do Projeto

Para executar o projeto, Ã© necessÃ¡rio instalar a seguinte biblioteca Python:

- Bokeh: Biblioteca para criar visualizaÃ§Ãµes interativas em navegadores da web.

```bash
pip install bokeh
```

### Como Executar o Projeto

1. Clone o repositÃ³rio do projeto.
2. Instale as dependÃªncias listadas acima.
3. Execute, no diretÃ³rio raiz do projeto, os comandos a seguir:

```console
foo@bar:~$ make clean
foo@bar:~$ make exec
```

### ConclusÃµes

> Podemos ver que para reconhecer a flor Iris Setosa e a diferenciar das outras o modelo Ã© quase que perfeito, porÃ©m entre as flores Iris Versicolor e Iris Virginica, o modelo nÃ£o conseguiu apresentar uma alta taxa de "certeza" para todos os dados de teste (que sÃ£o apresentados no console).\
Foi testado a rede neural com mais neurÃ´nios na camada interna, porÃ©m os resultados nÃ£o apresentaram uma melhora significativa, ainda que o modelo demorasse mais para ajustar os pesos. NÃ£o foi testado outros valores para a taxa de momento e a taxa de aprendizagem utilizada.


ğŸš€ğŸ”ğŸ’¡

---
